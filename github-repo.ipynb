import requests
import pandas as pd
from datetime import datetime, timedelta
import os
import time
import json

class GitHubCollector:
    def __init__(self, repo, token):
        self.repo = repo
        self.headers = {'Authorization': f'token {token}'}
        self.base_url = "https://api.github.com"
        self.output_dir = 'github_data'
        self.rate_limit_remaining = float('inf')
        self.rate_limit_reset = 0
        
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            
        # Create state file to track progress
        self.state_file = os.path.join(self.output_dir, 'collection_state.json')
        self.state = self._load_state()

    def _load_state(self):
        """Load or create collection state"""
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r') as f:
                return json.load(f)
        return {
            'commits_collected': 0,
            'commits_last_sha': None,
            'prs_collected': 0,
            'prs_last_number': None,
            'processed_developers': [],
            'processed_following': [],
            'last_collection_date': None
        }

    def _save_state(self):
        """Save collection state"""
        self.state['last_collection_date'] = datetime.now().isoformat()
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f)

    def _check_rate_limit(self):
        """Handle GitHub API rate limiting"""
        if self.rate_limit_remaining <= 1:
            wait_time = self.rate_limit_reset - time.time()
            if wait_time > 0:
                print(f"Rate limit reached. Waiting {wait_time:.0f} seconds...")
                time.sleep(wait_time + 1)

    def _make_request(self, url, params=None):
        """Make rate-limited request to GitHub API"""
        self._check_rate_limit()
        
        response = requests.get(url, headers=self.headers, params=params)
        
        # Update rate limit info
        self.rate_limit_remaining = int(response.headers.get('X-RateLimit-Remaining', 0))
        self.rate_limit_reset = int(response.headers.get('X-RateLimit-Reset', 0))
        
        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():
            self._check_rate_limit()
            return self._make_request(url, params)
            
        response.raise_for_status()
        return response

    def _get_paginated(self, url, params=None, max_items=None):
        """Handle GitHub API pagination with rate limiting and item limit"""
        items = []
        while url and (max_items is None or len(items) < max_items):
            response = self._make_request(url, params)
            data = response.json()
            
            if max_items:
                data = data[:max_items - len(items)]
            items.extend(data)
            
            if 'next' in response.links and (max_items is None or len(items) < max_items):
                url = response.links['next']['url']
                params = None
            else:
                break
        return items

    def collect_commits(self, since_days=180, max_commits=1000):
        """Collect commit data with file changes"""
        print(f"Collecting up to {max_commits} commits...")
        
        # Resume from last commit if exists
        if self.state['commits_collected'] >= max_commits:
            print("Already collected maximum number of commits.")
            return
        
        since_date = (datetime.now() - timedelta(days=since_days)).isoformat()
        url = f"{self.base_url}/repos/{self.repo}/commits"
        params = {'since': since_date}
        if self.state['commits_last_sha']:
            params['sha'] = self.state['commits_last_sha']
        
        commits_data = []
        files_data = []
        
        commits_to_collect = max_commits - self.state['commits_collected']
        for commit in self._get_paginated(url, params, max_items=commits_to_collect):
            if not commit.get('author') or not commit['author'].get('login'):
                continue
            
            # Get detailed commit info
            commit_detail = self._make_request(commit['url']).json()
            
            commits_data.append({
                'commit_hash': commit['sha'],
                'author': commit['author']['login'],
                'date': commit['commit']['author']['date'],
                'additions': commit_detail['stats'].get('additions', 0),
                'deletions': commit_detail['stats'].get('deletions', 0),
                'files_changed': len(commit_detail.get('files', []))
            })
            
            for file in commit_detail.get('files', []):
                files_data.append({
                    'commit_hash': commit['sha'],
                    'filename': file['filename'],
                    'status': file['status'],
                    'additions': file.get('additions', 0),
                    'deletions': file.get('deletions', 0),
                    'component': file['filename'].split('/')[0]
                })
            
            self.state['commits_collected'] += 1
            self.state['commits_last_sha'] = commit['sha']
            self._save_state()
        
        # Append to existing files if they exist
        self._save_data(commits_data, 'commits.csv')
        self._save_data(files_data, 'files_changed.csv')

    def collect_pull_requests(self, since_days=180, max_prs=500):
        """Collect PR data with reviews"""
        print(f"Collecting up to {max_prs} pull requests...")
        
        if self.state['prs_collected'] >= max_prs:
            print("Already collected maximum number of PRs.")
            return
        
        url = f"{self.base_url}/repos/{self.repo}/pulls"
        params = {
            'state': 'all',
            'sort': 'updated',
            'direction': 'desc',
            'per_page': 100
        }
        
        prs_data = []
        reviews_data = []
        cutoff_date = datetime.now() - timedelta(days=since_days)
        
        prs_to_collect = max_prs - self.state['prs_collected']
        for pr in self._get_paginated(url, params, max_items=prs_to_collect):
            updated_at = datetime.strptime(pr['updated_at'], '%Y-%m-%dT%H:%M:%SZ')
            if updated_at < cutoff_date:
                break
                
            prs_data.append({
                'pr_number': pr['number'],
                'author': pr['user']['login'],
                'created_at': pr['created_at'],
                'updated_at': pr['updated_at'],
                'closed_at': pr['closed_at'],
                'merged_at': pr['merged_at'],
                'state': pr['state']
            })
            
          
            
            self.state['prs_collected'] += 1
            self.state['prs_last_number'] = pr['number']
            self._save_state()
        
        self._save_data(prs_data, 'pull_requests.csv')
        self._save_data(reviews_data, 'pr_reviews.csv')

    def collect_developer_data(self, max_developers=200):
        """Collect developer profiles and relationships"""
        print(f"Collecting up to {max_developers} developer profiles...")
        
        # Get unique developers from existing files
        developers = set()
        for file in ['commits.csv', 'pull_requests.csv', 'pr_reviews.csv']:
            try:
                df = pd.read_csv(f"{self.output_dir}/{file}")
                if 'author' in df.columns:
                    developers.update(df['author'].unique())
                if 'reviewer' in df.columns:
                    developers.update(df['reviewer'].unique())
            except:
                continue
        
        # Limit the number of developers
        developers = list(developers)[:max_developers]
        
        # Collect profiles and following data
        for username in developers:
            # Skip if already processed
            if username in self.state['processed_developers']:
                continue
                
            try:
                # Get developer profile
                user_data = self._make_request(f"{self.base_url}/users/{username}").json()
                profile_data = [{
                    'username': username,
                    'location': user_data.get('location'),
                    'company': user_data.get('company'),
                    'created_at': user_data.get('created_at'),
                    'public_repos': user_data.get('public_repos'),
                    'followers': user_data.get('followers'),
                    'following': user_data.get('following')
                }]
                
                self._save_data(profile_data, 'developer_profiles.csv')
                
                # Get following relationships if not already processed
                if username not in self.state['processed_following']:
                    following = self._get_paginated(f"{self.base_url}/users/{username}/following")
                    following_data = [
                        {'follower': username, 'following': f['login']}
                        for f in following if f['login'] in developers
                    ]
                    
                    if following_data:
                        self._save_data(following_data, 'following_network.csv')
                    
                    self.state['processed_following'].append(username)
                
                self.state['processed_developers'].append(username)
                self._save_state()
                
            except Exception as e:
                print(f"Error processing developer {username}: {str(e)}")
                continue

    def create_review_network(self):
        """Create review relationship network from existing data"""
        print("Creating review relationship network...")
        try:
            reviews_df = pd.read_csv(f"{self.output_dir}/pr_reviews.csv")
            prs_df = pd.read_csv(f"{self.output_dir}/pull_requests.csv")
            
            relationships = []
            for _, review in reviews_df.iterrows():
                pr_author = prs_df[prs_df['pr_number'] == review['pr_number']]['author'].iloc[0]
                relationships.append({
                    'reviewer': review['reviewer'],
                    'author': pr_author,
                    'pr_number': review['pr_number'],
                    'review_state': review['state'],
                    'submitted_at': review['submitted_at']
                })
            
            pd.DataFrame(relationships).to_csv(f"{self.output_dir}/review_relationships.csv", index=False)
        except Exception as e:
            print(f"Error creating review relationships: {str(e)}")

    def _save_data(self, data, filename):
        """Save or append data to CSV file"""
        if not data:
            return
            
        df = pd.DataFrame(data)
        filepath = os.path.join(self.output_dir, filename)
        
        if os.path.exists(filepath):
            df.to_csv(filepath, mode='a', header=False, index=False)
        else:
            df.to_csv(filepath, index=False)

    def collect_all_data(self, since_days=180, max_commits=1000, max_prs=500, max_developers=200):
        """Collect all GitHub data with limits"""
        try:
            print(f"\nStarting data collection with limits:")
            print(f"- Max commits: {max_commits}")
            print(f"- Max PRs: {max_prs}")
            print(f"- Max developers: {max_developers}")
            print(f"- Time range: last {since_days} days")
            
            self.collect_commits(since_days, max_commits)
            self.collect_pull_requests(since_days, max_prs)
            self.collect_developer_data(max_developers)
            self.create_review_network()
            
            print("\nData collection complete!")
            print("Files saved in:", self.output_dir)
            
        except Exception as e:
            print(f"\nError during data collection: {str(e)}")
            print("You can resume the collection later - progress is saved in the state file.")

# Usage example:
if __name__ == "__main__":
    collector = GitHubCollector(
        repo="WordPress/wordpress-develop",
        token="*****"
    )
    
    # Collect data with limits
    collector.collect_all_data(
        since_days=180,      # Last 6 months
        max_commits=1000,    # Maximum 1000 commits
        max_prs=500,         # Maximum 500 PRs
        max_developers=200   # Maximum 200 developer profiles
    )
