{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting data collection with limits:\n",
      "- Max commits: 1000\n",
      "- Max PRs: 500\n",
      "- Max developers: 200\n",
      "- Time range: last 180 days\n",
      "Collecting up to 1000 commits...\n",
      "Collecting up to 500 pull requests...\n",
      "Already collected maximum number of PRs.\n",
      "Collecting up to 200 developer profiles...\n",
      "Creating review relationship network...\n",
      "Error creating review relationships: [Errno 2] No such file or directory: 'github_data/pr_reviews.csv'\n",
      "\n",
      "Data collection complete!\n",
      "Files saved in: github_data\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "class GitHubCollector:\n",
    "    def __init__(self, repo, token):\n",
    "        self.repo = repo\n",
    "        self.headers = {'Authorization': f'token {token}'}\n",
    "        self.base_url = \"https://api.github.com\"\n",
    "        self.output_dir = 'github_data'\n",
    "        self.rate_limit_remaining = float('inf')\n",
    "        self.rate_limit_reset = 0\n",
    "        \n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "            \n",
    "        # Create state file to track progress\n",
    "        self.state_file = os.path.join(self.output_dir, 'collection_state.json')\n",
    "        self.state = self._load_state()\n",
    "\n",
    "    def _load_state(self):\n",
    "        \"\"\"Load or create collection state\"\"\"\n",
    "        if os.path.exists(self.state_file):\n",
    "            with open(self.state_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\n",
    "            'commits_collected': 0,\n",
    "            'commits_last_sha': None,\n",
    "            'prs_collected': 0,\n",
    "            'prs_last_number': None,\n",
    "            'processed_developers': [],\n",
    "            'processed_following': [],\n",
    "            'last_collection_date': None\n",
    "        }\n",
    "\n",
    "    def _save_state(self):\n",
    "        \"\"\"Save collection state\"\"\"\n",
    "        self.state['last_collection_date'] = datetime.now().isoformat()\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(self.state, f)\n",
    "\n",
    "    def _check_rate_limit(self):\n",
    "        \"\"\"Handle GitHub API rate limiting\"\"\"\n",
    "        if self.rate_limit_remaining <= 1:\n",
    "            wait_time = self.rate_limit_reset - time.time()\n",
    "            if wait_time > 0:\n",
    "                print(f\"Rate limit reached. Waiting {wait_time:.0f} seconds...\")\n",
    "                time.sleep(wait_time + 1)\n",
    "\n",
    "    def _make_request(self, url, params=None):\n",
    "        \"\"\"Make rate-limited request to GitHub API\"\"\"\n",
    "        self._check_rate_limit()\n",
    "        \n",
    "        response = requests.get(url, headers=self.headers, params=params)\n",
    "        \n",
    "        # Update rate limit info\n",
    "        self.rate_limit_remaining = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "        self.rate_limit_reset = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "        \n",
    "        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n",
    "            self._check_rate_limit()\n",
    "            return self._make_request(url, params)\n",
    "            \n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "\n",
    "    def _get_paginated(self, url, params=None, max_items=None):\n",
    "        \"\"\"Handle GitHub API pagination with rate limiting and item limit\"\"\"\n",
    "        items = []\n",
    "        while url and (max_items is None or len(items) < max_items):\n",
    "            response = self._make_request(url, params)\n",
    "            data = response.json()\n",
    "            \n",
    "            if max_items:\n",
    "                data = data[:max_items - len(items)]\n",
    "            items.extend(data)\n",
    "            \n",
    "            if 'next' in response.links and (max_items is None or len(items) < max_items):\n",
    "                url = response.links['next']['url']\n",
    "                params = None\n",
    "            else:\n",
    "                break\n",
    "        return items\n",
    "\n",
    "    def collect_commits(self, since_days=180, max_commits=1000):\n",
    "        \"\"\"Collect commit data with file changes\"\"\"\n",
    "        print(f\"Collecting up to {max_commits} commits...\")\n",
    "        \n",
    "        # Resume from last commit if exists\n",
    "        if self.state['commits_collected'] >= max_commits:\n",
    "            print(\"Already collected maximum number of commits.\")\n",
    "            return\n",
    "        \n",
    "        since_date = (datetime.now() - timedelta(days=since_days)).isoformat()\n",
    "        url = f\"{self.base_url}/repos/{self.repo}/commits\"\n",
    "        params = {'since': since_date}\n",
    "        if self.state['commits_last_sha']:\n",
    "            params['sha'] = self.state['commits_last_sha']\n",
    "        \n",
    "        commits_data = []\n",
    "        files_data = []\n",
    "        \n",
    "        commits_to_collect = max_commits - self.state['commits_collected']\n",
    "        for commit in self._get_paginated(url, params, max_items=commits_to_collect):\n",
    "            if not commit.get('author') or not commit['author'].get('login'):\n",
    "                continue\n",
    "            \n",
    "            # Get detailed commit info\n",
    "            commit_detail = self._make_request(commit['url']).json()\n",
    "            \n",
    "            commits_data.append({\n",
    "                'commit_hash': commit['sha'],\n",
    "                'author': commit['author']['login'],\n",
    "                'date': commit['commit']['author']['date'],\n",
    "                'additions': commit_detail['stats'].get('additions', 0),\n",
    "                'deletions': commit_detail['stats'].get('deletions', 0),\n",
    "                'files_changed': len(commit_detail.get('files', []))\n",
    "            })\n",
    "            \n",
    "            for file in commit_detail.get('files', []):\n",
    "                files_data.append({\n",
    "                    'commit_hash': commit['sha'],\n",
    "                    'filename': file['filename'],\n",
    "                    'status': file['status'],\n",
    "                    'additions': file.get('additions', 0),\n",
    "                    'deletions': file.get('deletions', 0),\n",
    "                    'component': file['filename'].split('/')[0]\n",
    "                })\n",
    "            \n",
    "            self.state['commits_collected'] += 1\n",
    "            self.state['commits_last_sha'] = commit['sha']\n",
    "            self._save_state()\n",
    "        \n",
    "        # Append to existing files if they exist\n",
    "        self._save_data(commits_data, 'commits.csv')\n",
    "        self._save_data(files_data, 'files_changed.csv')\n",
    "\n",
    "    def collect_pull_requests(self, since_days=180, max_prs=500):\n",
    "        \"\"\"Collect PR data with reviews\"\"\"\n",
    "        print(f\"Collecting up to {max_prs} pull requests...\")\n",
    "        \n",
    "        if self.state['prs_collected'] >= max_prs:\n",
    "            print(\"Already collected maximum number of PRs.\")\n",
    "            return\n",
    "        \n",
    "        url = f\"{self.base_url}/repos/{self.repo}/pulls\"\n",
    "        params = {\n",
    "            'state': 'all',\n",
    "            'sort': 'updated',\n",
    "            'direction': 'desc',\n",
    "            'per_page': 100\n",
    "        }\n",
    "        \n",
    "        prs_data = []\n",
    "        reviews_data = []\n",
    "        cutoff_date = datetime.now() - timedelta(days=since_days)\n",
    "        \n",
    "        prs_to_collect = max_prs - self.state['prs_collected']\n",
    "        for pr in self._get_paginated(url, params, max_items=prs_to_collect):\n",
    "            updated_at = datetime.strptime(pr['updated_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            if updated_at < cutoff_date:\n",
    "                break\n",
    "                \n",
    "            prs_data.append({\n",
    "                'pr_number': pr['number'],\n",
    "                'author': pr['user']['login'],\n",
    "                'created_at': pr['created_at'],\n",
    "                'updated_at': pr['updated_at'],\n",
    "                'closed_at': pr['closed_at'],\n",
    "                'merged_at': pr['merged_at'],\n",
    "                'state': pr['state']\n",
    "            })\n",
    "            reviews_url = f\"{self.base_url}/repos/{self.repo}/pulls/{pr['number']}/reviews\"\n",
    "            try:\n",
    "                reviews = self._make_request(reviews_url).json()\n",
    "                for review in reviews:\n",
    "                    reviews_data.append({\n",
    "                        'pr_number': pr['number'],\n",
    "                        'reviewer': review['user']['login'],\n",
    "                        'state': review['state'],\n",
    "                        'submitted_at': review['submitted_at']\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching reviews for PR {pr['number']}: {str(e)}\")\n",
    "            \n",
    "          \n",
    "            \n",
    "            self.state['prs_collected'] += 1\n",
    "            self.state['prs_last_number'] = pr['number']\n",
    "            self._save_state()\n",
    "        \n",
    "        self._save_data(prs_data, 'pull_requests.csv')\n",
    "        self._save_data(reviews_data, 'pr_reviews.csv')\n",
    "\n",
    "    def collect_developer_data(self, max_developers=200):\n",
    "        \"\"\"Collect developer profiles and relationships\"\"\"\n",
    "        print(f\"Collecting up to {max_developers} developer profiles...\")\n",
    "        \n",
    "        # Get unique developers from existing files\n",
    "        developers = set()\n",
    "        for file in ['commits.csv', 'pull_requests.csv', 'pr_reviews.csv']:\n",
    "            try:\n",
    "                df = pd.read_csv(f\"{self.output_dir}/{file}\")\n",
    "                if 'author' in df.columns:\n",
    "                    developers.update(df['author'].unique())\n",
    "                if 'reviewer' in df.columns:\n",
    "                    developers.update(df['reviewer'].unique())\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Limit the number of developers\n",
    "        developers = list(developers)[:max_developers]\n",
    "        \n",
    "        # Collect profiles and following data\n",
    "        for username in developers:\n",
    "            # Skip if already processed\n",
    "            if username in self.state['processed_developers']:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Get developer profile\n",
    "                user_data = self._make_request(f\"{self.base_url}/users/{username}\").json()\n",
    "                profile_data = [{\n",
    "                    'username': username,\n",
    "                    'location': user_data.get('location'),\n",
    "                    'company': user_data.get('company'),\n",
    "                    'created_at': user_data.get('created_at'),\n",
    "                    'public_repos': user_data.get('public_repos'),\n",
    "                    'followers': user_data.get('followers'),\n",
    "                    'following': user_data.get('following')\n",
    "                }]\n",
    "                \n",
    "                self._save_data(profile_data, 'developer_profiles.csv')\n",
    "                \n",
    "                # Get following relationships if not already processed\n",
    "                if username not in self.state['processed_following']:\n",
    "                    following = self._get_paginated(f\"{self.base_url}/users/{username}/following\")\n",
    "                    following_data = [\n",
    "                        {'follower': username, 'following': f['login']}\n",
    "                        for f in following if f['login'] in developers\n",
    "                    ]\n",
    "                    \n",
    "                    if following_data:\n",
    "                        self._save_data(following_data, 'following_network.csv')\n",
    "                    \n",
    "                    self.state['processed_following'].append(username)\n",
    "                \n",
    "                self.state['processed_developers'].append(username)\n",
    "                self._save_state()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing developer {username}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def create_review_network(self):\n",
    "        \"\"\"Create review relationship network from existing data\"\"\"\n",
    "        print(\"Creating review relationship network...\")\n",
    "        try:\n",
    "            reviews_df = pd.read_csv(f\"{self.output_dir}/pr_reviews.csv\")\n",
    "            prs_df = pd.read_csv(f\"{self.output_dir}/pull_requests.csv\")\n",
    "            \n",
    "            relationships = []\n",
    "            for _, review in reviews_df.iterrows():\n",
    "                pr_author = prs_df[prs_df['pr_number'] == review['pr_number']]['author'].iloc[0]\n",
    "                relationships.append({\n",
    "                    'reviewer': review['reviewer'],\n",
    "                    'author': pr_author,\n",
    "                    'pr_number': review['pr_number'],\n",
    "                    'review_state': review['state'],\n",
    "                    'submitted_at': review['submitted_at']\n",
    "                })\n",
    "            \n",
    "            pd.DataFrame(relationships).to_csv(f\"{self.output_dir}/review_relationships.csv\", index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating review relationships: {str(e)}\")\n",
    "\n",
    "    def _save_data(self, data, filename):\n",
    "        \"\"\"Save or append data to CSV file\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "            \n",
    "        df = pd.DataFrame(data)\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            df.to_csv(filepath, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df.to_csv(filepath, index=False)\n",
    "\n",
    "    def collect_all_data(self, since_days=180, max_commits=1000, max_prs=500, max_developers=200):\n",
    "        \"\"\"Collect all GitHub data with limits\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nStarting data collection with limits:\")\n",
    "            print(f\"- Max commits: {max_commits}\")\n",
    "            print(f\"- Max PRs: {max_prs}\")\n",
    "            print(f\"- Max developers: {max_developers}\")\n",
    "            print(f\"- Time range: last {since_days} days\")\n",
    "            \n",
    "            self.collect_commits(since_days, max_commits)\n",
    "            self.collect_pull_requests(since_days, max_prs)\n",
    "            self.collect_developer_data(max_developers)\n",
    "            self.create_review_network()\n",
    "            \n",
    "            print(\"\\nData collection complete!\")\n",
    "            print(\"Files saved in:\", self.output_dir)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during data collection: {str(e)}\")\n",
    "            print(\"You can resume the collection later - progress is saved in the state file.\")\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    collector = GitHubCollector(\n",
    "        repo=\"WordPress/wordpress-develop\",\n",
    "        token=\"token id"\"\n",
    "    )\n",
    "    \n",
    "    # Collect data with limits\n",
    "    collector.collect_all_data(\n",
    "        since_days=180,      # Last 6 months\n",
    "        max_commits=1000,    # Maximum 1000 commits\n",
    "        max_prs=500,         # Maximum 500 PRs\n",
    "        max_developers=200   # Maximum 200 developer profiles\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
